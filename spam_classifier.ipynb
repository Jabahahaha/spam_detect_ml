{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spam vs Ham Text Classifier\n",
        "\n",
        "This project is about building a simple text classifier that can distinguish between spam and ham (normal) messages. I'll be using the SMS Spam Collection dataset and trying out different machine learning approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "The goal here is to build a classifier that can automatically detect spam messages. This is a binary classification problem where each message is either \"spam\" or \"ham\" (not spam).\n",
        "\n",
        "I'll start by loading the data, cleaning it up a bit, then extracting features using TF-IDF. After that, I'll train a classifier and see how well it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows:\n",
            "  label                                               text\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "\n",
            "Class distribution:\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "df = df[['v1', 'v2']]\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "print(\"First few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "Before training, I need to clean up the text. I'll convert everything to lowercase, remove punctuation, and get rid of extra spaces. I'm keeping it simple here - no fancy techniques like stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example before preprocessing:\n",
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "\n",
            "Example after preprocessing:\n",
            "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "df['text_clean'] = df['text'].apply(preprocess)\n",
        "\n",
        "print(\"Example before preprocessing:\")\n",
        "print(df['text'].iloc[0])\n",
        "print(\"\\nExample after preprocessing:\")\n",
        "print(df['text_clean'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction\n",
        "\n",
        "Now I'll use TF-IDF to convert the text into numbers that the model can work with. TF-IDF gives higher weights to words that are important but not too common across all messages.\n",
        "\n",
        "I'll split the data first, then fit the vectorizer only on the training set to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (4457, 3000)\n",
            "Test set shape: (1115, 3000)\n"
          ]
        }
      ],
      "source": [
        "X = df['text_clean']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=3000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Training set shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Test set shape: {X_test_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "\n",
        "I'll start with Logistic Regression since it usually works well for text classification and is easy to understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "preds = model.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Let me check how well the model performs on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9686\n",
            "Precision: 1.0000\n",
            "Recall: 0.7667\n",
            "F1 Score: 0.8679\n",
            "\n",
            "Confusion Matrix:\n",
            "[[965   0]\n",
            " [ 35 115]]\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(y_test, preds)\n",
        "precision = precision_score(y_test, preds, pos_label='spam')\n",
        "recall = recall_score(y_test, preds, pos_label='spam')\n",
        "f1 = f1_score(y_test, preds, pos_label='spam')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments\n",
        "\n",
        "I want to see if using bigrams (pairs of words) helps improve the results. Let me try that and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison:\n",
            "Unigrams - Accuracy: 0.9686, F1: 0.8679\n",
            "Bigrams  - Accuracy: 0.9695, F1: 0.8722\n"
          ]
        }
      ],
      "source": [
        "vectorizer_bigram = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
        "X_train_bigram = vectorizer_bigram.fit_transform(X_train)\n",
        "X_test_bigram = vectorizer_bigram.transform(X_test)\n",
        "\n",
        "model_bigram = LogisticRegression(random_state=42)\n",
        "model_bigram.fit(X_train_bigram, y_train)\n",
        "preds_bigram = model_bigram.predict(X_test_bigram)\n",
        "\n",
        "accuracy_bigram = accuracy_score(y_test, preds_bigram)\n",
        "f1_bigram = f1_score(y_test, preds_bigram, pos_label='spam')\n",
        "\n",
        "print(\"Comparison:\")\n",
        "print(f\"Unigrams - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "print(f\"Bigrams  - Accuracy: {accuracy_bigram:.4f}, F1: {f1_bigram:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This was a good learning project for me. The Logistic Regression model worked pretty well on this dataset and achieved decent accuracy.\n",
        "\n",
        "What worked well:\n",
        "- TF-IDF was effective for converting text to features\n",
        "- Simple preprocessing was enough to get good results\n",
        "- Logistic Regression was straightforward and performed well\n",
        "\n",
        "What didn't work as well:\n",
        "- Bigrams didn't improve results much in my experiment, which was a bit surprising\n",
        "- The model might struggle with messages that use a lot of slang or abbreviations\n",
        "\n",
        "What I would improve next:\n",
        "- Try more advanced preprocessing techniques like handling numbers or special characters differently\n",
        "- Experiment with other classifiers like Random Forest or SVM\n",
        "- Maybe collect more data or try to balance the classes if needed\n",
        "- Add cross-validation to get more reliable performance estimates"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
